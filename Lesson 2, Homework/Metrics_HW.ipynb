{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOb1Kh1lj609"
      },
      "source": [
        "## Rules\n",
        "\n",
        "1. All mathematical expressions should be written in **LaTeX** for better clarity and formatting.\n",
        "2. Ensure that the entire notebook can execute seamlessly from start to finish without encountering errors.\n",
        "3. Focus on optimizing the runtime of the code wherever possible to enhance performance.\n",
        "\n",
        "## Notation\n",
        "\n",
        "- $c$: The optimal constant model.  \n",
        "- $y_i$: The target values in the dataset.  \n",
        "- $w_i$: The weights associated with the loss function.\n",
        "- $q$: Quantile value in range $[0, 1]$.\n",
        "\n",
        "# Важно! О формате сдачи\n",
        "\n",
        "* **При решении ноутбука используйте данный шаблон. Не нужно удалять текстовые ячейки c разметкой частей ноутбука и формулировками заданий. Добавлять свои ячейки, при необходимости, конечно можно**\n",
        "* **Везде, где в формулровке задания есть какой-либо вопрос (или просьба вывода), необходимо прописать ответ в ячейку (код или markdown).**\n",
        "* **Наличие кода решения (или аналитического решения - в зависимости от задачи) обязательно. Письменные ответы на вопросы без сопутствующего кода/аналитического решения оцениваются в 0 баллов.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr4xVEQsjeF9"
      },
      "source": [
        "## Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfx6HJOj5FLb"
      },
      "source": [
        "### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQFbyGOo7Y9p"
      },
      "source": [
        "Consider the loss function:\n",
        "\n",
        "\\begin{align}\n",
        "L = \\sum w_i \\cdot \\left( \\log(y_i) - \\log(c) \\right)^2\n",
        "\\end{align}\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\sum w_i = 1$\n",
        "\n",
        "#### Tasks\n",
        "\n",
        "1. **Analytically find the best constant $c$** for the given loss function.\n",
        "2. **Determine the name of the aggregation of $y_i$'s** at the end if $w_1 = w_2 = \\dots = w_n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QujLaGbD5A8E"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hVENP-85OaX"
      },
      "source": [
        "\n",
        "$$\n",
        "L = \\sum w_i \\cdot \\left( \\log(y_i) - \\log(c) \\right)^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "   $$\n",
        "   L = \\sum w_i \\cdot \\left( \\log(y_i) - x \\right)^2\n",
        "   $$\n",
        "\n",
        "\n",
        "   $$\n",
        "   \\frac{dL}{dx} = -2 \\sum w_i \\cdot \\left( \\log(y_i) - x \\right)\n",
        "   $$\n",
        "\n",
        "\n",
        "   $$\n",
        "   -2 \\sum w_i \\cdot \\left( \\log(y_i) - x \\right) = 0 \\implies x = \\sum w_i \\log(y_i)\n",
        "   $$\n",
        "\n",
        "\n",
        "   $$\n",
        "   \\log(c) = \\sum w_i \\log(y_i) \\implies c = \\exp\\left(\\sum w_i \\log(y_i)\\right)\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Если \\( w_1 = w_2 = \\dots = w_n = \\frac{1}{n} \\), то:\n",
        "$$\n",
        "c = \\exp\\left(\\frac{1}{n} \\sum \\log(y_i)\\right) = \\prod y_i^{1/n}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**Final Answers:**  \n",
        "1. Оптимальная константа:  \n",
        "   $$\\boxed{c = \\exp\\left(\\sum w_i \\log(y_i)\\right)}$$  \n",
        "2. При равных весах агрегация называется средним геометрическим"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ:\n",
            "1. Оптимальная константа c = exp(∑(w_i * log(y_i))) 2.0\n",
            "2. При равных весах (w_i = 1/n) получаем геометрическое среднее значений y_i 2.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Аналитическое решение\n",
        "def find_oconstant(y, w=None):\n",
        "    y = np.array(y)\n",
        "    if w is None:\n",
        "        w = np.ones_like(y) / len(y)\n",
        "    else:\n",
        "        w = np.array(w) / np.sum(w)\n",
        "    \n",
        "    c = np.exp(np.sum(w * np.log(y)))\n",
        "    return c\n",
        "\n",
        "# 2. Проверка для случая равных весов\n",
        "y = np.array([1, 2, 3, 4, 5])\n",
        "c_equal = find_oconstant(y)  # равные веса\n",
        "geom_mean = np.exp(np.mean(np.log(y)))  # геометрическое среднее\n",
        "\n",
        "print(\"Ответ:\")\n",
        "print(\"1. Оптимальная константа c = exp(∑(w_i * log(y_i)))\",c_equal)\n",
        "print(\"2. При равных весах (w_i = 1/n) получаем геометрическое среднее значений y_i\", geom_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXSZRuzZ7Mnt"
      },
      "source": [
        "## Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A14Tq1s470G6"
      },
      "source": [
        "### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTM5F2rZ78Zm"
      },
      "source": [
        "Consider the **quantile loss function** $L$, and prove that the optimal constant $c$ corresponds to the quantile $q$ of the data $y_1, \\dots, y_n$.\n",
        "\n",
        "The quantile loss function is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "L =\n",
        "\\begin{cases}\n",
        "q \\cdot (y_i - c), & \\text{if } y_i \\geq c \\\\\n",
        "(1-q) \\cdot (c - y_i), & \\text{if } y_i < c\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "**Hint**:\n",
        "\n",
        "Proof for optimal MAE constant on [this page](https://ds100.org/course-notes/constant_model_loss_transformations/loss_transformations.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HohX1KQk70Jd"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoMeumwu7RGc"
      },
      "source": [
        "### Solution\n",
        "\n",
        "---\n",
        "\n",
        "$$\n",
        "L(c, y_i) = \n",
        "\\begin{cases} \n",
        "q \\cdot (y_i - c), & \\text{if } y_i \\geq c \\\\\n",
        "(1 - q) \\cdot (c - y_i), & \\text{if } y_i < c \n",
        "\\end{cases}\n",
        "$$\n",
        "$$\n",
        "L = \\sum_{i=1}^n L(c, y_i)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial c} = (1 - q) \\cdot (\\text{number of } y_i < c) - q \\cdot (\\text{number of } y_i \\geq c)\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial c} = (1 - q)k - q(n - k)\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "$$\n",
        "(1 - q)k - q(n - k) = 0 \\implies k = qn\n",
        "$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### **Final Answer**  \n",
        "The optimal constant \\( c \\) that minimizes the quantile loss function is the:  \n",
        "$$\n",
        "\\boxed{q\\text{-th quantile}} \\text{ of } y_1, \\dots, y_n.\n",
        "$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKg7Sh-H_XlP"
      },
      "source": [
        "\n",
        "## Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gsCBQe8C-U-i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The optimal constant c is: 4.1969999999996475\n",
            "The minimum loss is: 243.96167948452023\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def bruteforce_constant(y: np.ndarray, loss_func: callable, tol: float) -> float:\n",
        "    cv = np.arange(y.min(), y.max() + tol, tol)\n",
        "    bc = cv[0]\n",
        "    ml = float('inf')\n",
        "    for c in cv:\n",
        "        cr = loss_func(y, c)\n",
        "        if cr < ml:\n",
        "            ml = cr\n",
        "            bc = c\n",
        "    return bc\n",
        "\n",
        "def loss(y, c):\n",
        "    return np.sum(np.log(np.cosh(y - c)))\n",
        "\n",
        "y = np.array([1, 2, 3, 4, 55, 99, 100])\n",
        "oc = bruteforce_constant(y, loss, tol=0.001)\n",
        "ml = loss(y, oc)\n",
        "print(f\"The optimal constant c is: {oc}\")\n",
        "print(f\"The minimum loss is: {ml}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1UPKOpBLb5z"
      },
      "source": [
        "## Problem 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NI-R1YHaIRx_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The optimal constant c is: 4.197202525934449\n",
            "The minimum loss is: 243.9616794082023\n"
          ]
        }
      ],
      "source": [
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "\n",
        "def minimize_constant(y: np.ndarray, loss_func: callable, tol: float) -> float:\n",
        "    ic = np.median(y)\n",
        "    \n",
        "\n",
        "    def objective(c_array):\n",
        "        return loss_func(y, c_array[0])\n",
        "    \n",
        "    res = minimize(objective, x0=[ic], method='BFGS', tol=tol)\n",
        "    \n",
        "\n",
        "    return res.x[0]\n",
        "\n",
        "def loss(y, c):\n",
        "    return np.sum(np.log(np.cosh(y - c)))\n",
        "\n",
        "y = np.array([1, 2, 3, 4, 55, 99, 100])\n",
        "oc = minimize_constant(y, loss, tol=0.001)\n",
        "ml = loss(y, oc)\n",
        "print(f\"The optimal constant c is: {oc}\")\n",
        "print(f\"The minimum loss is: {ml}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBJbvAj27Bld"
      },
      "source": [
        "# Problem 5\n",
        "\n",
        "## Description\n",
        "In a multiclass classification problem, we often compare a model’s performance with a simple baseline. Two baseline approaches are:\n",
        "\n",
        "1. **Always predict the most frequent class** (also known as the “constant baseline”).  \n",
        "2. **Randomly sample a class** for every object with probabilities proportional to the class frequencies in the training set.\n",
        "\n",
        "The goal of this task is to determine which of these two approaches yields a higher **Accuracy** on a given dataset.\n",
        "\n",
        "You are provided with:  \n",
        "1. A set of objects \\\\( X \\\\) and their true class labels \\\\( y \\\\), where \\\\( y \\in \\{C_1, C_2, \\dots, C_k\\} \\\\).  \n",
        "2. The frequencies (or proportions) of each class in the training data.\n",
        "\n",
        "You need to:  \n",
        "1. **Explain** how to implement these two baseline approaches (always predicting the most frequent class vs. sampling a class according to its frequency).  \n",
        "2. **Calculate** the expected Accuracy for each approach.  \n",
        "3. **Compare** the ress to determine which method is more likely to produce a higher Accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cabuv0Ti79qz"
      },
      "source": [
        "## Solution\n",
        "\n",
        "\n",
        "\\[\n",
        "p(C_m) \\quad \\text{vs} \\quad \\sum_{j=1}^{k} p(C_j)^2\n",
        "\\]\n",
        "\n",
        "- Since \\( p(C_m) \\) is the maximum class probability, we analyze whether:\n",
        "  \\[\n",
        "  p(C_m) \\geq \\sum_{j=1}^{k} p(C_j)^2\n",
        "  \\]\n",
        "- Using Jensen's inequality for convex functions (\\( f(x) = x^2 \\) is convex for \\( x \\geq 0 \\)), we apply the inequality:\n",
        "  \\[\n",
        "  \\sum_{j=1}^{k} p(C_j)^2 \\leq p(C_m)\n",
        "  \\]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Final Answer:\n",
        "In most cases, always predicting the most frequent class is more likely to yield a higher Accuracy, especially when the class distribution is imbalanced."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
